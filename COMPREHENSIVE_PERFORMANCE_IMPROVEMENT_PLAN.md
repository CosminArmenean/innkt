# üìä Comprehensive Performance Improvement Plan
## INNKT Platform - Microservices Optimization Strategy

**Date:** October 9, 2025  
**Status:** Strategic Planning Phase  
**Priority:** High Impact, Medium Effort

---

## üéØ Executive Summary

This document outlines a comprehensive performance improvement strategy across all INNKT microservices. Current analysis shows **4-6 second user lookup times** in the Groups service, primarily due to:
1. Lack of caching layer in Groups service
2. Sequential HTTP calls to Officer service
3. No batch processing for multiple user lookups
4. Inefficient service-to-service communication

**Expected Performance Gains:**
- **User lookups**: 4-6s ‚Üí 50-200ms (95-98% improvement)
- **API response times**: 10-15s ‚Üí 500ms-2s (80-90% improvement)
- **Database load**: 60-70% reduction
- **Network calls**: 80-90% reduction

---

## üìà Current Performance Analysis

### üî¥ **Critical Issues (Immediate Action Required)**

#### 1. **Groups Service - No Caching Layer**
**Current State:**
- Every user lookup makes an HTTP call to Officer service (4-6 seconds)
- Sequential calls for multiple users (2+ users = 8-12 seconds)
- No Redis integration
- No in-memory caching

**Impact:**
```
Invite User Operation:
‚îú‚îÄ Permission check: 100ms
‚îú‚îÄ User lookup #1 (lisbon.teresa): 4,300ms ‚ùå
‚îú‚îÄ User lookup #2 (junior11): 2,100ms ‚ùå
‚îú‚îÄ Database operations: 200ms
‚îî‚îÄ Kafka event: 50ms
TOTAL: ~10,000ms+ (TIMEOUT)
```

**Location:** `Backend/innkt.Groups/Services/UserService.cs`

---

#### 2. **Officer Service - Slow User Profile Queries**
**Current State:**
- No query optimization
- Missing composite indexes on `AspNetUsers` table
- No prepared statements for common queries
- No connection pooling optimization

**Impact:**
- Average query time: 2-4 seconds per user lookup
- Database CPU spikes during high traffic
- Connection pool exhaustion under load

**Location:** `Backend/innkt.Officer/Services/AuthService.cs`

---

#### 3. **Sequential HTTP Calls in All Services**
**Current State:**
- Groups service makes sequential calls instead of batch requests
- Social service has batch support but not utilized everywhere
- Messaging service makes individual user lookups

**Impact:**
```
Loading 10 users sequentially:
10 users √ó 4s = 40 seconds ‚ùå

With batching:
10 users in 1 batch = 500ms ‚úÖ
```

---

### üü° **Medium Priority Issues (Short-term Optimization)**

#### 4. **Social Service - Underutilized Caching**
**Current State:**
- Excellent multi-layer caching implemented (`UserProfileCacheService`)
- **BUT:** Other services don't leverage it
- Cache hit rate unknown (no monitoring)
- Cache invalidation strategy incomplete

**Location:** `Backend/innkt.Social/Services/UserProfileCacheService.cs`

---

#### 5. **Database Query Optimization**
**Current State:**
- Good indexing in Officer service
- Missing composite indexes for complex queries
- No query result caching
- N+1 query problems in some endpoints

**Impact:**
- 20-30% slower queries than optimal
- Unnecessary database load
- Higher hosting costs

---

#### 6. **Frontend Timeout Configuration**
**Current State:**
- ‚úÖ **FIXED:** Increased from 10s to 30s
- Still masking underlying performance issues
- Should be optimized to 5s or less

---

### üü¢ **Low Priority Issues (Long-term Optimization)**

#### 7. **Monitoring & Observability**
**Current State:**
- Basic logging
- No performance metrics collection
- No distributed tracing
- No cache hit rate monitoring

#### 8. **API Gateway & Rate Limiting**
**Current State:**
- Direct service-to-service calls
- No centralized rate limiting
- No request deduplication
- No circuit breaker pattern

---

## üöÄ Implementation Plan

### **Phase 1: Critical Fixes (Week 1-2) - HIGH IMPACT**

#### ‚úÖ **Task 1.1: Implement Caching in Groups Service**
**Priority:** üî¥ CRITICAL  
**Effort:** Medium (4-6 hours)  
**Impact:** 95% performance improvement

**Implementation:**

1. **Add Redis Support to Groups Service**
   ```csharp
   // Backend/innkt.Groups/Services/UserProfileCacheService.cs
   public class UserProfileCacheService : IUserProfileCacheService
   {
       private readonly IMemoryCache _memoryCache;
       private readonly IDistributedCache _distributedCache;
       private readonly IUserService _userService;
       private readonly ILogger<UserProfileCacheService> _logger;
       
       // Layer 1: Memory Cache (5 min, 500 users) - Fastest
       // Layer 2: Redis Cache (1 hour) - Fast
       // Layer 3: Officer Service via Social Service - Source of truth
   }
   ```

2. **Update Dependencies**
   ```xml
   <!-- Backend/innkt.Groups/innkt.Groups.csproj -->
   <PackageReference Include="Microsoft.Extensions.Caching.Memory" Version="9.0.0" />
   <PackageReference Include="Microsoft.Extensions.Caching.StackExchangeRedis" Version="9.0.0" />
   ```

3. **Configure Redis in Program.cs**
   ```csharp
   // Add Memory Cache
   builder.Services.AddMemoryCache(options =>
   {
       options.SizeLimit = 500; // Cache up to 500 user profiles
   });
   
   // Add Redis Distributed Cache
   builder.Services.AddStackExchangeRedisCache(options =>
   {
       options.Configuration = "localhost:6379";
       options.InstanceName = "InNKT:Groups:";
   });
   
   // Register cache service
   builder.Services.AddScoped<IUserProfileCacheService, UserProfileCacheService>();
   ```

**Expected Results:**
- User lookups: 4-6s ‚Üí 50-200ms
- Cache hit rate: 70-80% (estimated)
- 95% reduction in HTTP calls to Officer service

---

#### ‚úÖ **Task 1.2: Implement Batch User Lookups**
**Priority:** üî¥ CRITICAL  
**Effort:** Small (2-3 hours)  
**Impact:** 90% improvement for multi-user operations

**Implementation:**

1. **Update UserService in Groups**
   ```csharp
   public async Task<Dictionary<Guid, UserBasicInfo>> GetUsersBatchAsync(List<Guid> userIds)
   {
       var result = new Dictionary<Guid, UserBasicInfo>();
       var uncachedIds = new List<Guid>();
       
       // Check cache first
       foreach (var userId in userIds)
       {
           var cached = await _cacheService.GetUserProfileAsync(userId);
           if (cached != null)
           {
               result[userId] = cached;
           }
           else
           {
               uncachedIds.Add(userId);
           }
       }
       
       // Batch fetch uncached users
       if (uncachedIds.Any())
       {
           var batchResult = await _userService.GetUsersBasicInfoAsync(uncachedIds);
           foreach (var user in batchResult)
           {
               result[user.Id] = user;
               await _cacheService.StoreUserProfileAsync(user);
           }
       }
       
       return result;
   }
   ```

2. **Update InviteUserAsync to use batch loading**
   ```csharp
   // Instead of:
   var invitedByUser = await _userService.GetUserBasicInfoAsync(userId);
   
   // Use:
   var users = await _userService.GetUsersBatchAsync(new List<Guid> { userId, invitedUserId });
   var invitedByUser = users[userId];
   ```

**Expected Results:**
- Multi-user operations: 40s ‚Üí 500ms
- Single HTTP call instead of N calls
- Better database utilization

---

#### ‚úÖ **Task 1.3: Optimize Officer Service Queries**
**Priority:** üî¥ CRITICAL  
**Effort:** Small (2-3 hours)  
**Impact:** 50-70% improvement in database queries

**Implementation:**

1. **Add Composite Indexes**
   ```sql
   -- Backend/innkt.Officer/Migrations/AddCompositeIndexes.sql
   
   -- User lookup by ID (most common operation)
   CREATE INDEX IF NOT EXISTS IX_AspNetUsers_Id_IsActive 
   ON AspNetUsers(Id, IsActive) 
   WHERE IsActive = 1;
   
   -- User lookup by username
   CREATE INDEX IF NOT EXISTS IX_AspNetUsers_Username_IsActive 
   ON AspNetUsers(UserName, IsActive) 
   WHERE IsActive = 1;
   
   -- Batch user lookups
   CREATE INDEX IF NOT EXISTS IX_AspNetUsers_Id_Username_Email 
   ON AspNetUsers(Id) 
   INCLUDE (UserName, Email, IsVerified);
   ```

2. **Implement Query Result Caching in AuthService**
   ```csharp
   public async Task<UserProfileDto?> GetUserByIdAsync(Guid userId)
   {
       var cacheKey = $"user:profile:{userId}";
       
       // Try cache first
       var cached = await _cache.GetStringAsync(cacheKey);
       if (!string.IsNullOrEmpty(cached))
       {
           return JsonSerializer.Deserialize<UserProfileDto>(cached);
       }
       
       // Query database with optimized query
       var user = await _context.Users
           .AsNoTracking() // Read-only, faster
           .Where(u => u.Id == userId.ToString() && u.IsActive)
           .Select(u => new UserProfileDto
           {
               Id = u.Id,
               Username = u.UserName,
               Email = u.Email,
               IsVerified = u.IsVerified,
               // ... other fields
           })
           .FirstOrDefaultAsync();
       
       if (user != null)
       {
           // Cache for 1 hour
           await _cache.SetStringAsync(cacheKey, JsonSerializer.Serialize(user), 
               new DistributedCacheEntryOptions
               {
                   AbsoluteExpirationRelativeToNow = TimeSpan.FromHours(1)
               });
       }
       
       return user;
   }
   ```

**Expected Results:**
- Database query time: 2-4s ‚Üí 50-100ms
- 60-70% reduction in database load
- Better connection pool utilization

---

### **Phase 2: Medium Priority Optimizations (Week 3-4)**

#### ‚úÖ **Task 2.1: Centralized User Profile Service**
**Priority:** üü° MEDIUM  
**Effort:** Large (8-12 hours)  
**Impact:** Long-term architecture improvement

**Strategy:**
Instead of each service calling Officer directly, route all user profile requests through the Social service's `UserProfileCacheService`.

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Groups Service  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                       ‚îú‚îÄ‚îÄ‚ñ∫ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ    ‚îÇ Social Service        ‚îÇ
‚îÇMessaging Service‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ UserProfileCacheService‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îÇ  ‚îú‚îÄ Memory Cache      ‚îÇ
                       ‚îÇ    ‚îÇ  ‚îú‚îÄ Redis Cache       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ    ‚îÇ  ‚îî‚îÄ Officer Service  ‚îÇ
‚îÇ Other Services  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Benefits:**
- Single source of truth for caching
- Consistent cache invalidation
- Reduced duplication
- Better monitoring

---

#### ‚úÖ **Task 2.2: Implement Connection Pooling Optimization**
**Priority:** üü° MEDIUM  
**Effort:** Small (1-2 hours)  
**Impact:** 15-20% improvement under load

**Implementation:**
```csharp
// appsettings.json for all services
{
  "ConnectionStrings": {
    "DefaultConnection": "Host=localhost;Database=innkt_groups;Username=admin_officer;Password=CAvp57rt26;Pooling=true;MinPoolSize=10;MaxPoolSize=100;ConnectionIdleLifetime=30;ConnectionPruningInterval=10"
  }
}
```

**Redis Connection Pooling:**
```csharp
builder.Services.AddStackExchangeRedisCache(options =>
{
    options.Configuration = "localhost:6379";
    options.InstanceName = "InNKT:";
    
    // Connection pooling
    var configOptions = ConfigurationOptions.Parse("localhost:6379");
    configOptions.AbortOnConnectFail = false;
    configOptions.ConnectRetry = 3;
    configOptions.ConnectTimeout = 5000;
    configOptions.SyncTimeout = 5000;
    options.ConfigurationOptions = configOptions;
});
```

---

#### ‚úÖ **Task 2.3: Implement Response Compression**
**Priority:** üü° MEDIUM  
**Effort:** Trivial (30 minutes)  
**Impact:** 40-60% reduction in response size

**Implementation:**
```csharp
// Add to all service Program.cs
builder.Services.AddResponseCompression(options =>
{
    options.EnableForHttps = true;
    options.Providers.Add<GzipCompressionProvider>();
    options.Providers.Add<BrotliCompressionProvider>();
});

builder.Services.Configure<GzipCompressionProviderOptions>(options =>
{
    options.Level = CompressionLevel.Fastest;
});

// Use in pipeline
app.UseResponseCompression();
```

---

### **Phase 3: Long-term Optimizations (Month 2+)**

#### ‚úÖ **Task 3.1: Implement Monitoring & Metrics**
**Priority:** üü¢ LOW (but important)  
**Effort:** Large (12-16 hours)  
**Impact:** Visibility and proactive optimization

**Tools to Integrate:**
1. **Application Insights** or **Prometheus + Grafana**
2. **Distributed Tracing** (OpenTelemetry)
3. **Cache Hit Rate Monitoring**
4. **Database Query Performance Tracking**

**Key Metrics to Track:**
- Cache hit/miss rates
- Average response times per endpoint
- Database query times
- HTTP call durations
- Error rates
- Request throughput

---

#### ‚úÖ **Task 3.2: API Gateway Pattern**
**Priority:** üü¢ LOW  
**Effort:** Very Large (20-30 hours)  
**Impact:** Architecture improvement, scalability

**Implementation:**
- Introduce **Ocelot** or **YARP** as API Gateway
- Centralized rate limiting
- Request/response caching at gateway level
- Circuit breaker pattern
- Request deduplication

---

#### ‚úÖ **Task 3.3: Database Read Replicas**
**Priority:** üü¢ LOW  
**Effort:** Medium (6-8 hours + infrastructure)  
**Impact:** Horizontal scalability for read-heavy operations

**Strategy:**
- PostgreSQL read replicas for Officer database
- Route read queries to replicas
- Keep writes on primary
- Eventual consistency acceptable for user profiles

---

## üìä Expected Performance Improvements

### **After Phase 1 (Week 1-2)**

| Operation | Before | After | Improvement |
|-----------|--------|-------|-------------|
| Single user lookup | 4-6s | 50-200ms | **95-98%** ‚úÖ |
| Invite user (2 lookups) | 10-12s | 500ms-1s | **92-95%** ‚úÖ |
| Load 10 group members | 40-60s | 500ms-1s | **98%** ‚úÖ |
| Group list with members | 30-45s | 2-3s | **90-93%** ‚úÖ |
| Database query time | 2-4s | 50-100ms | **95-98%** ‚úÖ |

### **After Phase 2 (Week 3-4)**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Cache hit rate | 0% | 75-85% | **New capability** ‚úÖ |
| HTTP calls to Officer | 100% | 15-25% | **75-85% reduction** ‚úÖ |
| Database connections | 50-100 | 10-30 | **70-80% reduction** ‚úÖ |
| Response size | 100KB | 40-60KB | **40-60% reduction** ‚úÖ |
| Memory usage | Baseline | +100MB | **Acceptable trade-off** ‚ö†Ô∏è |

### **After Phase 3 (Month 2+)**

| Capability | Before | After | Improvement |
|------------|--------|-------|-------------|
| Observability | ‚ùå None | ‚úÖ Full monitoring | **New capability** ‚úÖ |
| Scalability | Limited | Horizontal | **Unlimited scale** ‚úÖ |
| Failure handling | Basic | Circuit breakers | **Better resilience** ‚úÖ |
| API management | Distributed | Centralized | **Easier maintenance** ‚úÖ |

---

## üéØ Quick Wins (Can be done today)

### **1. Enable Redis in Groups Service (2-3 hours)**
```bash
cd Backend/innkt.Groups
dotnet add package Microsoft.Extensions.Caching.StackExchangeRedis
dotnet add package Microsoft.Extensions.Caching.Memory
```

Add to `Program.cs`:
```csharp
builder.Services.AddMemoryCache();
builder.Services.AddStackExchangeRedisCache(options =>
{
    options.Configuration = "localhost:6379";
    options.InstanceName = "InNKT:Groups:";
});
```

### **2. Add Database Indexes (30 minutes)**
```sql
-- Run in PostgreSQL
CREATE INDEX IF NOT EXISTS IX_AspNetUsers_Id_IsActive 
ON "AspNetUsers"("Id", "IsActive") 
WHERE "IsActive" = TRUE;
```

### **3. Enable Response Compression (15 minutes)**
```csharp
// Add to all services
builder.Services.AddResponseCompression();
```

---

## üîß Testing Strategy

### **Performance Testing Checklist**
- [ ] Benchmark current performance (baseline)
- [ ] Test single user lookup
- [ ] Test batch user lookup (10, 50, 100 users)
- [ ] Test invite flow end-to-end
- [ ] Test group member loading
- [ ] Load test with 100 concurrent requests
- [ ] Monitor cache hit rates
- [ ] Check database connection pool usage
- [ ] Verify memory usage is acceptable

### **Tools**
- **JMeter** or **k6** for load testing
- **SQL Profiler** for database query analysis
- **Redis CLI** for cache monitoring
- **dotnet-counters** for .NET performance metrics

---

## üí∞ Resource Requirements

### **Development Time**
- **Phase 1:** 10-15 hours (1-2 developers)
- **Phase 2:** 15-20 hours (1-2 developers)
- **Phase 3:** 30-50 hours (2-3 developers)

### **Infrastructure**
- ‚úÖ **Redis**: Already deployed in Docker
- ‚úÖ **PostgreSQL**: Already deployed
- ‚ö†Ô∏è **Monitoring**: May require additional resources
- ‚ö†Ô∏è **Read Replicas**: Additional database instances

### **Cost Impact**
- **Phase 1:** $0 (uses existing infrastructure)
- **Phase 2:** $0 (uses existing infrastructure)
- **Phase 3:** ~$50-100/month (monitoring tools + read replicas)

---

## üìù Implementation Checklist

### **Phase 1: Critical Fixes (Week 1-2)**
- [ ] Add Redis caching to Groups service
- [ ] Implement UserProfileCacheService in Groups
- [ ] Add batch user lookup endpoints
- [ ] Update InviteUserAsync to use batching
- [ ] Add composite indexes to AspNetUsers
- [ ] Implement query result caching in Officer
- [ ] Test and benchmark improvements
- [ ] Deploy to production

### **Phase 2: Medium Priority (Week 3-4)**
- [ ] Create centralized user profile service architecture
- [ ] Optimize connection pooling across all services
- [ ] Implement response compression
- [ ] Add HTTP caching headers
- [ ] Update all services to use centralized caching
- [ ] Test and benchmark improvements
- [ ] Deploy to production

### **Phase 3: Long-term (Month 2+)**
- [ ] Set up Application Insights or Prometheus
- [ ] Implement distributed tracing
- [ ] Create monitoring dashboards
- [ ] Implement API Gateway (Ocelot/YARP)
- [ ] Add circuit breaker pattern
- [ ] Set up database read replicas
- [ ] Implement rate limiting
- [ ] Load testing and optimization

---

## üéì Best Practices Going Forward

### **1. Caching Strategy**
- **Always check cache first** before making HTTP calls
- **Cache user profiles** for 1 hour (invalidate on update)
- **Cache database query results** for frequently accessed data
- **Use multi-layer caching** (Memory ‚Üí Redis ‚Üí Database)

### **2. Database Optimization**
- **Use `AsNoTracking()`** for read-only queries
- **Batch database operations** whenever possible
- **Avoid N+1 queries** by using eager loading
- **Index all foreign keys** and commonly queried columns

### **3. API Design**
- **Provide batch endpoints** for multi-item operations
- **Use pagination** for large result sets
- **Implement ETag/Last-Modified** for conditional requests
- **Return only necessary data** (use DTOs)

### **4. Monitoring**
- **Track cache hit rates** to optimize cache configuration
- **Monitor database query times** to identify slow queries
- **Log all HTTP calls** with duration metrics
- **Set up alerts** for performance degradation

---

## üìö References & Documentation

### **Internal Documentation**
- `PROFILE_PICTURE_OPTIMIZATION_PLAN.md` - Related caching strategies
- `Backend/innkt.Social/Services/UserProfileCacheService.cs` - Reference implementation
- `MONGODB_SECURITY_GUIDE.md` - Database security best practices

### **External Resources**
- [ASP.NET Core Performance Best Practices](https://docs.microsoft.com/en-us/aspnet/core/performance/performance-best-practices)
- [Redis Caching Patterns](https://redis.io/docs/manual/patterns/)
- [Entity Framework Core Performance](https://docs.microsoft.com/en-us/ef/core/performance/)
- [PostgreSQL Indexing Strategies](https://www.postgresql.org/docs/current/indexes.html)

---

## ‚úÖ Success Criteria

### **Phase 1 Success Metrics**
- ‚úÖ User lookup < 200ms (95th percentile)
- ‚úÖ Invite operation < 2s end-to-end
- ‚úÖ Cache hit rate > 70%
- ‚úÖ Database query time < 100ms average
- ‚úÖ No timeout errors in normal operation

### **Phase 2 Success Metrics**
- ‚úÖ All services using centralized caching
- ‚úÖ Response compression enabled everywhere
- ‚úÖ Connection pooling optimized
- ‚úÖ 80% reduction in Officer service calls

### **Phase 3 Success Metrics**
- ‚úÖ Full observability dashboard
- ‚úÖ < 1% error rate under load
- ‚úÖ Horizontal scalability demonstrated
- ‚úÖ Circuit breakers preventing cascading failures

---

## üö® Risks & Mitigation

### **Risk 1: Cache Invalidation Issues**
**Mitigation:**
- Implement event-driven cache invalidation
- Use TTL as fallback
- Monitor stale data occurrences

### **Risk 2: Increased Memory Usage**
**Mitigation:**
- Set cache size limits
- Monitor memory usage
- Use LRU eviction policy

### **Risk 3: Cache Stampede**
**Mitigation:**
- Implement cache warming
- Use stale-while-revalidate pattern
- Add random jitter to TTL

### **Risk 4: Complexity Increase**
**Mitigation:**
- Comprehensive documentation
- Clear monitoring and debugging tools
- Gradual rollout with feature flags

---

**Document Version:** 1.0  
**Last Updated:** October 9, 2025  
**Next Review:** After Phase 1 completion  
**Owner:** Development Team

